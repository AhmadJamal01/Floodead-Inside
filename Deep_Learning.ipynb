{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyN6FFdo361PBn/UOclS8EPO",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadJamal01/Floodead-Inside/blob/main/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lU65wIk57haw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "aZmn6TyYyb2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdal > /dev/null"
      ],
      "metadata": {
        "id": "rArTdndiyA4z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# import gdown\n",
        "# gdown.download(\"https://drive.google.com/uc?id=1och-QmNa3FAiS-wssgzCwISbmpSezIi_\", \"dataset.zip\", quiet=False)\n",
        "# gdown.extractall(\"dataset.zip\")\n",
        "# path = 'dataset/'\n",
        "\n",
        "# gdown.download(\"https://drive.google.com/file/d/1YUbTBFrk9QF0ivR5F640G3dhCMC3XQUZ/view?usp=sharing\", \"dataset.zip\", quiet=False, fuzzy=True)"
      ],
      "metadata": {
        "id": "a7XRy_rtyFEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "import zipfile\n",
        "\n",
        "# Mount your Google Drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "# Specify the path to your zip file in Google Drive\n",
        "zip_path = '/content/drive/MyDrive/dataset.zip'\n",
        "\n",
        "# Extract the zip file\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall('/content/dataset/')  # Specify the extraction path\n",
        "\n",
        "# Access the extracted files\n",
        "path = '/content/dataset/'  # Update with your extraction path\n",
        "# You can now work with the extracted files in the extracted_files_path directory\n"
      ],
      "metadata": {
        "id": "6VnV8Tvt7zX-",
        "outputId": "88ab79ba-d855-41e9-d03a-91c90a997c33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "RgAykbJbytzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root=path, transform=data_transforms)\n",
        "\n",
        "# Split the dataset into train and test\n",
        "TRAIN_SIZE = 0.8\n",
        "VALIDAtION_SIZE = 0.1\n",
        "\n",
        "train_size = int(TRAIN_SIZE * len(dataset))\n",
        "validation_size = int(VALIDAtION_SIZE * len(dataset))\n",
        "test_size = len(dataset) - (train_size + validation_size)\n",
        "train, validation, test = torch.utils.data.random_split(dataset, (train_size, validation_size, test_size))\n",
        "\n",
        "batch_size = 32\n",
        "trainLoader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validationLoader = torch.utils.data.DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "testLoader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZevE8yKh8uDO"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device"
      ],
      "metadata": {
        "id": "AerkruVB8J6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Availabe device for training is: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwYdp7KR8NcC",
        "outputId": "eb7ec150-f13f-4664-a76a-5be1ac343c35"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Availabe device for training is: cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "wpqTzs1Ry4ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet18"
      ],
      "metadata": {
        "id": "B40QUlXjgiMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ResNet model \n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "#  Modify the first layer to be able to handle our data\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer to work with two classes\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "# Move the model to GPU if possible\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and lr schehduler\n",
        "# criterion: The loss function used for optimization.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer: The optimizer to update the model's parameters.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# step_lr_scheduler: The learning rate scheduler for adjusting the learning rate during training.\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "dRTZBJdo84PR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet"
      ],
      "metadata": {
        "id": "FLmmBKMBgkAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DensNet model \n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer to be able to handle our data\n",
        "model.features.conv0 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer to work with two classes\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 2)\n",
        "\n",
        "# Move the model to GPU if possible\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and lr schehduler\n",
        "# criterion: The loss function used for optimization.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer: The optimizer to update the model's parameters.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# step_lr_scheduler: The learning rate scheduler for adjusting the learning rate during training.\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "GGmEpy04glmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Resenet50"
      ],
      "metadata": {
        "id": "I-Ku6mYlnTqb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ResNet model \n",
        "model = models.resnet50(pretrained=True)\n",
        "\n",
        "# Modify the last layer to work with two classes\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "# Move the model to GPU if possible\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and lr scheduler\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vl68F1cynVu4",
        "outputId": "3a4eaa38-bf16-49e0-d1ce-44d499d936b6"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Enhancement "
      ],
      "metadata": {
        "id": "C0_KeWHH6lCE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropout\n",
        "Dropout is a regularization technique that can help prevent overfitting by randomly disabling neurons during training. It can be added after fully connected layers to reduce the model's reliance on specific features and encourage better generalization."
      ],
      "metadata": {
        "id": "cFK3W61W60wt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Add Dropout after the last fully connected layer\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Sequential(\n",
        "    nn.Linear(num_features, 512),\n",
        "    nn.ReLU(),\n",
        "    nn.Dropout(0.5),\n",
        "    nn.Linear(512, 2)\n",
        ")\n",
        "# use model.classifier for the densenet\n"
      ],
      "metadata": {
        "id": "xRvdF9kX65DE"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "nLdO2X4OzB7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_model(model, trainloader, validationloader, device, criterion, optimizer, step_lr_scheduler, epochs):\n",
        "    '''\n",
        "    This code is designed to train a model, track its performance during training, and return the best model along with the loss and accuracy values.\n",
        "    '''\n",
        "    try:\n",
        "        model = model.to(device)\n",
        "\n",
        "        since = time.time()\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_acc = 0.0\n",
        "\n",
        "        train_acc = []\n",
        "        train_loss = []\n",
        "        valid_acc = []\n",
        "        valid_loss = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print('-' * 80)\n",
        "            print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "            for phase in ['train', 'valid']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                    dataloader = trainloader\n",
        "                    dataset_size = len(trainloader.dataset)\n",
        "                else:\n",
        "                    model.eval()\n",
        "                    dataloader = validationloader\n",
        "                    dataset_size = len(validationloader.dataset)\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                for data in dataloader:\n",
        "                    # Get the training data items of the current batch\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Forward pass\n",
        "                        outputs = model(inputs)\n",
        "                        # Predict the current batch\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        # Compute loss\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            # Set the gradients of all the parameters of the model to zero\n",
        "                            optimizer.zero_grad()\n",
        "                            # Backward propagation to calculate the gradient\n",
        "                            loss.backward()\n",
        "                            # Update the NN weights by using the gradient\n",
        "                            optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    step_lr_scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_size\n",
        "                epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "                if phase == 'valid':\n",
        "                    valid_acc.append(epoch_acc)\n",
        "                    valid_loss.append(epoch_loss)\n",
        "                else:\n",
        "                    train_acc.append(epoch_acc)\n",
        "                    train_loss.append(epoch_loss)\n",
        "\n",
        "                print('{} loss: {:.4f} --------------- {} accuracy: {:.4f}'.format(phase, epoch_loss, phase, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'valid' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('-' * 80)\n",
        "        print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best validation accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "        # load best model weights\n",
        "        print('Loading final model weights...')\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        return model, train_loss, train_acc, valid_loss, valid_acc\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        return None, [], [], [], []\n"
      ],
      "metadata": {
        "id": "I3hgpYOs6v0S"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, train_acc, valid_loss, valid_acc = train_model(model, trainLoader, validationLoader, device, criterion, optimizer, step_lr_scheduler, epochs=2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42zWQDkt6pIQ",
        "outputId": "55f41a3f-4222-43ff-dc64-db67334b60ab"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/2\n",
            "train loss: 0.0121 --------------- train accuracy: 1.0000\n",
            "valid loss: 0.0105 --------------- valid accuracy: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2/2\n",
            "train loss: 0.0102 --------------- train accuracy: 1.0000\n",
            "valid loss: 0.0073 --------------- valid accuracy: 1.0000\n",
            "--------------------------------------------------------------------------------\n",
            "Training completed in 26m 54s\n",
            "Best validation accuracy: 1.000000\n",
            "Loading final model weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Cxb3uKWfzHuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloader, device):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            \n",
        "    return np.asarray(y_pred), np.asarray(y_true)"
      ],
      "metadata": {
        "id": "v8j754lVfBY7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Test the model on the test dataset\n",
        "y_pred, y_true = predict(model, testLoader, device)\n",
        "        \n",
        "# Evalaution        \n",
        "# Calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
        "print('F1 Score: {:.2f}'.format(f1))\n",
        "\n",
        "# Show the whole report\n",
        "classification_report = classification_report(y_true, y_pred)\n",
        "print(classification_report)\n"
      ],
      "metadata": {
        "id": "sWtfEoTz878B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fb6dc137-666a-4c35-9ace-39612d2f5e28"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 100.00%\n",
            "F1 Score: 0.00\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        93\n",
            "\n",
            "    accuracy                           1.00        93\n",
            "   macro avg       1.00      1.00      1.00        93\n",
            "weighted avg       1.00      1.00      1.00        93\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/metrics/_classification.py:1609: UndefinedMetricWarning: F-score is ill-defined and being set to 0.0 due to no true nor predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, \"true nor predicted\", \"F-score is\", len(true_sum))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "0sUsorg-zKNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'resnet50.pth')"
      ],
      "metadata": {
        "id": "0sQBavjx89oE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFO2bINtyrzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}