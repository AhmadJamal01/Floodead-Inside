{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMTUlcfnIDvY1QtRqA+UXs3",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AhmadJamal01/Floodead-Inside/blob/main/Deep_Learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "lU65wIk57haw"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import numpy as np\n",
        "import torch.nn as nn\n",
        "import torchvision.transforms as transforms\n",
        "import torch.utils.data as data\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision import models, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import lr_scheduler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read the dataset"
      ],
      "metadata": {
        "id": "aZmn6TyYyb2K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gdal > /dev/null"
      ],
      "metadata": {
        "id": "rArTdndiyA4z"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gdown\n",
        "gdown.download(\"https://drive.google.com/uc?id=1och-QmNa3FAiS-wssgzCwISbmpSezIi_\", \"dataset.zip\", quiet=False)\n",
        "gdown.extractall(\"dataset.zip\")\n",
        "path = 'dataset/'\n"
      ],
      "metadata": {
        "id": "a7XRy_rtyFEm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare the data"
      ],
      "metadata": {
        "id": "RgAykbJbytzD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data_transforms = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.485, 0.456, 0.406), (0.229, 0.224, 0.225))\n",
        "])\n",
        "\n",
        "dataset = torchvision.datasets.ImageFolder(root=path, transform=data_transforms)\n",
        "\n",
        "# Split the dataset into train and test\n",
        "TRAIN_SIZE = 0.8\n",
        "VALIDAtION_SIZE = 0.1\n",
        "\n",
        "train_size = int(TRAIN_SIZE * len(dataset))\n",
        "validation_size = int(VALIDAtION_SIZE * len(dataset))\n",
        "test_size = len(dataset) - (train_size + validation_size)\n",
        "train, validation, test = torch.utils.data.random_split(dataset, (train_size, validation_size, test_size))\n",
        "\n",
        "batch_size = 32\n",
        "trainLoader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True, num_workers=2)\n",
        "validationLoader = torch.utils.data.DataLoader(validation, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "testLoader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, num_workers=2)\n",
        "\n"
      ],
      "metadata": {
        "id": "ZevE8yKh8uDO"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Device"
      ],
      "metadata": {
        "id": "AerkruVB8J6-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Availabe device for training is: {device}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kwYdp7KR8NcC",
        "outputId": "ccfa6bd6-0899-422e-8385-2fdd121f15f2"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Availabe device for training is: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model Architecture"
      ],
      "metadata": {
        "id": "wpqTzs1Ry4ux"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ResNet"
      ],
      "metadata": {
        "id": "B40QUlXjgiMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define ResNet model \n",
        "model = models.resnet18(pretrained=True)\n",
        "\n",
        "#  Modify the first layer to be able to handle our data\n",
        "model.conv1 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer to work with two classes\n",
        "num_features = model.fc.in_features\n",
        "model.fc = nn.Linear(num_features, 2)\n",
        "\n",
        "# Move the model to GPU if possible\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and lr schehduler\n",
        "# criterion: The loss function used for optimization.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer: The optimizer to update the model's parameters.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# step_lr_scheduler: The learning rate scheduler for adjusting the learning rate during training.\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)\n",
        "\n"
      ],
      "metadata": {
        "id": "dRTZBJdo84PR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "14977364-f65a-4857-fe20-970c5e38e515"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to /root/.cache/torch/hub/checkpoints/resnet18-f37072fd.pth\n",
            "100%|██████████| 44.7M/44.7M [00:00<00:00, 326MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DenseNet"
      ],
      "metadata": {
        "id": "FLmmBKMBgkAD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define DensNet model \n",
        "model = models.densenet121(pretrained=True)\n",
        "\n",
        "# Modify the first layer to be able to handle our data\n",
        "model.features.conv0 = nn.Conv2d(3, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
        "\n",
        "# Modify the last layer to work with two classes\n",
        "model.classifier = nn.Linear(model.classifier.in_features, 2)\n",
        "\n",
        "# Move the model to GPU if possible\n",
        "model = model.to(device)\n",
        "\n",
        "# Define loss function, optimizer, and lr schehduler\n",
        "# criterion: The loss function used for optimization.\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# optimizer: The optimizer to update the model's parameters.\n",
        "optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "# step_lr_scheduler: The learning rate scheduler for adjusting the learning rate during training.\n",
        "step_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size=7, gamma=0.1)"
      ],
      "metadata": {
        "id": "GGmEpy04glmW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Training"
      ],
      "metadata": {
        "id": "nLdO2X4OzB7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import copy\n",
        "\n",
        "def train_model(model, trainloader, validationloader, device, criterion, optimizer, step_lr_scheduler, epochs):\n",
        "    '''\n",
        "    This code is designed to train a model, track its performance during training, and return the best model along with the loss and accuracy values.\n",
        "    '''\n",
        "    try:\n",
        "        model = model.to(device)\n",
        "\n",
        "        since = time.time()\n",
        "        best_model_wts = copy.deepcopy(model.state_dict())\n",
        "        best_acc = 0.0\n",
        "\n",
        "        train_acc = []\n",
        "        train_loss = []\n",
        "        valid_acc = []\n",
        "        valid_loss = []\n",
        "\n",
        "        for epoch in range(epochs):\n",
        "            print('-' * 80)\n",
        "            print(f'Epoch {epoch + 1}/{epochs}')\n",
        "\n",
        "            for phase in ['train', 'valid']:\n",
        "                if phase == 'train':\n",
        "                    model.train()\n",
        "                    dataloader = trainloader\n",
        "                    dataset_size = len(trainloader.dataset)\n",
        "                else:\n",
        "                    model.eval()\n",
        "                    dataloader = validationloader\n",
        "                    dataset_size = len(validationloader.dataset)\n",
        "\n",
        "                running_loss = 0.0\n",
        "                running_corrects = 0\n",
        "\n",
        "                for data in dataloader:\n",
        "                    # Get the training data items of the current batch\n",
        "                    inputs, labels = data\n",
        "                    inputs, labels = inputs.to(device), labels.to(device)\n",
        "\n",
        "                    with torch.set_grad_enabled(phase == 'train'):\n",
        "                        # Forward pass\n",
        "                        outputs = model(inputs)\n",
        "                        # Predict the current batch\n",
        "                        _, preds = torch.max(outputs, 1)\n",
        "                        # Compute loss\n",
        "                        loss = criterion(outputs, labels)\n",
        "\n",
        "                        if phase == 'train':\n",
        "                            # Set the gradients of all the parameters of the model to zero\n",
        "                            optimizer.zero_grad()\n",
        "                            # Backward propagation to calculate the gradient\n",
        "                            loss.backward()\n",
        "                            # Update the NN weights by using the gradient\n",
        "                            optimizer.step()\n",
        "\n",
        "                    running_loss += loss.item() * inputs.size(0)\n",
        "                    running_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "                if phase == 'train':\n",
        "                    step_lr_scheduler.step()\n",
        "\n",
        "                epoch_loss = running_loss / dataset_size\n",
        "                epoch_acc = running_corrects.double() / dataset_size\n",
        "\n",
        "                if phase == 'valid':\n",
        "                    valid_acc.append(epoch_acc)\n",
        "                    valid_loss.append(epoch_loss)\n",
        "                else:\n",
        "                    train_acc.append(epoch_acc)\n",
        "                    train_loss.append(epoch_loss)\n",
        "\n",
        "                print('{} loss: {:.4f} --------------- {} accuracy: {:.4f}'.format(phase, epoch_loss, phase, epoch_acc))\n",
        "\n",
        "                # deep copy the model\n",
        "                if phase == 'valid' and epoch_acc > best_acc:\n",
        "                    best_acc = epoch_acc\n",
        "                    best_model_wts = copy.deepcopy(model.state_dict())\n",
        "\n",
        "        time_elapsed = time.time() - since\n",
        "        print('-' * 80)\n",
        "        print('Training completed in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
        "        print('Best validation accuracy: {:4f}'.format(best_acc))\n",
        "\n",
        "        # load best model weights\n",
        "        print('Loading final model weights...')\n",
        "        model.load_state_dict(best_model_wts)\n",
        "        return model, train_loss, train_acc, valid_loss, valid_acc\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"An error occurred during training: {str(e)}\")\n",
        "        return None, [], [], [], []\n"
      ],
      "metadata": {
        "id": "I3hgpYOs6v0S"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model, train_loss, train_acc, valid_loss, valid_acc = train_model(model, trainLoader, validationLoader, device, criterion, optimizer, step_lr_scheduler, epochs=15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "42zWQDkt6pIQ",
        "outputId": "813e31f4-6a84-4d38-e3f9-223886e4d4d4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--------------------------------------------------------------------------------\n",
            "Epoch 1/15\n",
            "train loss: 0.5632 --------------- train accuracy: 0.7028\n",
            "valid loss: 0.4304 --------------- valid accuracy: 0.7500\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 2/15\n",
            "train loss: 0.3146 --------------- train accuracy: 0.8657\n",
            "valid loss: 0.3538 --------------- valid accuracy: 0.8478\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 3/15\n",
            "train loss: 0.2266 --------------- train accuracy: 0.9132\n",
            "valid loss: 0.2498 --------------- valid accuracy: 0.8804\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 4/15\n",
            "train loss: 0.1426 --------------- train accuracy: 0.9579\n",
            "valid loss: 0.1843 --------------- valid accuracy: 0.9565\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 5/15\n",
            "train loss: 0.1246 --------------- train accuracy: 0.9647\n",
            "valid loss: 0.1648 --------------- valid accuracy: 0.9457\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 6/15\n",
            "train loss: 0.1174 --------------- train accuracy: 0.9566\n",
            "valid loss: 0.2263 --------------- valid accuracy: 0.9022\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 7/15\n",
            "train loss: 0.0837 --------------- train accuracy: 0.9674\n",
            "valid loss: 0.2148 --------------- valid accuracy: 0.9348\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 8/15\n",
            "train loss: 0.0807 --------------- train accuracy: 0.9729\n",
            "valid loss: 0.1665 --------------- valid accuracy: 0.9348\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 9/15\n",
            "train loss: 0.0558 --------------- train accuracy: 0.9864\n",
            "valid loss: 0.1561 --------------- valid accuracy: 0.9457\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 10/15\n",
            "train loss: 0.0501 --------------- train accuracy: 0.9905\n",
            "valid loss: 0.1683 --------------- valid accuracy: 0.9348\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 11/15\n",
            "train loss: 0.0392 --------------- train accuracy: 0.9919\n",
            "valid loss: 0.1433 --------------- valid accuracy: 0.9565\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 12/15\n",
            "train loss: 0.0468 --------------- train accuracy: 0.9905\n",
            "valid loss: 0.1322 --------------- valid accuracy: 0.9565\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 13/15\n",
            "train loss: 0.0420 --------------- train accuracy: 0.9919\n",
            "valid loss: 0.1352 --------------- valid accuracy: 0.9565\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 14/15\n",
            "train loss: 0.0392 --------------- train accuracy: 0.9905\n",
            "valid loss: 0.1694 --------------- valid accuracy: 0.9457\n",
            "--------------------------------------------------------------------------------\n",
            "Epoch 15/15\n",
            "train loss: 0.0351 --------------- train accuracy: 0.9932\n",
            "valid loss: 0.1434 --------------- valid accuracy: 0.9457\n",
            "--------------------------------------------------------------------------------\n",
            "Training completed in 21m 18s\n",
            "Best validation accuracy: 0.956522\n",
            "Loading final model weights...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluation"
      ],
      "metadata": {
        "id": "Cxb3uKWfzHuC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predict(model, dataloader, device):\n",
        "    y_pred = []\n",
        "    y_true = []\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in dataloader:\n",
        "            inputs, labels = inputs.to(device), labels.to(device)\n",
        "            outputs = model(inputs)\n",
        "\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            \n",
        "            y_pred.extend(predicted.cpu().numpy())\n",
        "            y_true.extend(labels.cpu().numpy())\n",
        "            \n",
        "    return np.asarray(y_pred), np.asarray(y_true)"
      ],
      "metadata": {
        "id": "v8j754lVfBY7"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import accuracy_score, f1_score\n",
        "\n",
        "# Test the model on the test dataset\n",
        "y_pred, y_true = predict(model, testLoader, device)\n",
        "        \n",
        "# Evalaution        \n",
        "# Calculate accuracy and F1 score\n",
        "accuracy = accuracy_score(y_true, y_pred)\n",
        "f1 = f1_score(y_true, y_pred)\n",
        "print('Accuracy: {:.2f}%'.format(accuracy * 100))\n",
        "print('F1 Score: {:.2f}'.format(f1))\n",
        "\n",
        "# Show the whole report\n",
        "classification_report = classification_report(y_true, y_pred)\n",
        "print(classification_report)\n"
      ],
      "metadata": {
        "id": "sWtfEoTz878B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a9765ebc-916b-4dd3-f2db-43539162231e"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 95.70%\n",
            "F1 Score: 0.96\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.98      0.93      0.95        43\n",
            "           1       0.94      0.98      0.96        50\n",
            "\n",
            "    accuracy                           0.96        93\n",
            "   macro avg       0.96      0.96      0.96        93\n",
            "weighted avg       0.96      0.96      0.96        93\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Saving the model"
      ],
      "metadata": {
        "id": "0sUsorg-zKNq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# save model\n",
        "torch.save(model.state_dict(), 'resnet18.pth')"
      ],
      "metadata": {
        "id": "0sQBavjx89oE"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lFO2bINtyrzH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}